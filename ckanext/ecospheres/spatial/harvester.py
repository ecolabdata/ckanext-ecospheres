# encoding: utf-8

from datetime import datetime
from lxml import etree
import requests

import ckan.plugins as plugins
import ckan.plugins.toolkit as toolkit
from ckan.model.package import Package
from ckan.lib.helpers import json

from ckanext.spatial.interfaces import ISpatialHarvester
from ckanext.harvest.harvesters.base import HarvesterBase

from ckanext.ecospheres.spatial.utils import build_dataset_dict_from_schema
from ckanext.ecospheres.spatial.maps import ISO_639_2

from ckanext.ecospheres.vocabulary.reader import get_uri_from_label, \
    get_uri_from_identifier


class FrSpatialHarvester(plugins.SingletonPlugin):
    '''Customization of spatial metadata harvest.
    
    See :py:class:`ckanext.spatial.harvesters.base.SpatialHarvester`.
    
    '''

    plugins.implements(ISpatialHarvester, inherit=True)
    
    def get_package_dict(self, context, data_dict):
        '''Add some usefull informations to package metadata dictionnary.
        
        :param context: contains a reference to the model, eg to
            perform DB queries, and the user name used for
            authorization.
        :type context: dict
        :param data_dict: available data. Contains four keys:
            * `package_dict`
               The default package_dict generated by the harvester. Modify this
               or create a brand new one.
            * `iso_values`
               The parsed ISO XML document values. These contain more fields
               that are not added by default to the ``package_dict``.
            * `xml_tree`
               The full XML etree object. If some values not present in
               ``iso_values`` are needed, these can be extracted via xpath.
            * `harvest_object`
               A ``HarvestObject`` domain object which contains a reference
               to the original metadata document (``harvest_object.content``)
               and the harvest source (``harvest_object.source``).
        :type data_dict: dict
        
        :returns: a dataset dict ready to be used by ``package_create`` or
                  ``package_update``
        :rtype: dict
        
        See :py:func:`ckanext.spatial.harvesters.base.SpatialHarvester.get_package_dict`
        and :py:func:`ckanext.spatial.interface.ISpatialHarvester.get_package_dict`.
        
        '''
        package_dict = data_dict['package_dict']
        iso_values = data_dict['iso_values'] 
        xml_tree = data_dict['xml_tree']

        language = iso_values.get('metadata-language') or 'fr'
        if len(language) > 2:
            # d'autant que de besoin, conversion des codes de langue
            # sur 3 caractères en codes sur 2 caractères, comme attendu en RDF
            language=ISO_639_2.get(language, language)
        
        dataset_dict = build_dataset_dict_from_schema('dataset', language=language)

        # --- various metadata from package_dict ---
        for target_field, package_field in {
            # dataset_dict key -> package_dict key
            'owner_org': 'owner_org'
        }:
            dataset_dict.set_value(target_field, package_dict.get(package_field))

        # --- various metadata from package_dict's extras ---
        extras_map = {
            # ckanext-spatial extras key -> dataset_dict key
            'graphic-preview-file': 'graphic_preview'   
        }
        for elem in package_dict['extras']:
            if elem['key'] in extras_map:
                dataset_dict.set_value(extras_map[elem['key']], elem['value'])

        # --- various metadata from iso_values ---
        for target_field, iso_field in {
            # dataset_dict key -> iso_values key
            'free_tag': 'tags',
            'title': 'title',
            'notes': 'abstract',
            'name': 'guid',
            'provenance': 'lineage',
            'provenance': 'maintenance-note', # TODO: provenance or version_info ? 
            'identifier': 'unique-resource-identifier'
        }:
            dataset_dict.set_value(target_field, iso_values.get(iso_field))
        # NB: package name should always be its guid, for easier
        # handling of packages relationships and duplicate removal

        if not dataset_dict.get('title'):
            dataset_dict.set_value('title', iso_values.get('alternate-title'))

        name = dataset_dict.get('name')

        # --- dates ----
        if iso_values.get('dataset-reference-date'):
            type_date_map = {
                # ISO CI_DateTypeCode -> dataset_dict key
                'creation': 'created',
                'publication': 'issued',
                'revision': 'modified',
            }
            for date_object in iso_values['dataset-reference-date']:
                if date_object['type'] in type_date_map:
                    dataset_dict.set_value(type_date_map[date_object['type']], date_object['value'])
        
        if iso_values.get('temporal-extent-begin') or iso_values.get('temporal-extent-end'):
            temporal_dict = dataset_dict.new_item('temporal')
            temporal_dict.set_value('start_date', iso_values.get('temporal-extent-begin'))
            temporal_dict.set_value('end_date', iso_values.get('temporal-extent-end'))

        # --- organisations ---
        if iso_values.get('responsible-organisation'):
            base_role_map = {
                # ISO CI_RoleCode -> dataset_dict key
                'owner': 'rights_holder',
                'publisher': 'publisher',
                'author': 'creator',
                'pointOfContact': 'contact_point'
                }
            for org_object in iso_values['responsible-organisation']:
                if not 'role' in org_object or not 'organisation-name' in org_object:
                    continue
                org_role = org_object['role']
                if org_role in base_role_map:
                    org_dict = dataset_dict.new_item(base_role_map[org_role])
                else:
                    role_uri = get_uri_from_identifier('inspire_role', org_role)
                    if role_uri:
                        qa_dict = dataset_dict.new_item('qualified_attribution')
                        qa_dict.set_value('had_role', role_uri)
                        org_dict = qa_dict.new_item('agent')
                    else:
                        continue
                org_dict.set_value('name', org_object['organisation-name'])
                if 'contact-info' in org_object:
                    org_dict.set_value('email', org_object['contact-info'].get('email'))
                    org_dict.set_value('url', org_object['contact-info'].get('online-resource'))
        
        # --- metadata's metadata ---
        meta_dict = dataset_dict.new_item('is_primary_topic_of')
        meta_dict.set_value('harvested', datetime.now().astimezone().isoformat())
        meta_dict.set_value('modified', iso_values.get('metadata-date'))
        meta_dict.set_value('identifier', name)

        meta_language = iso_values.get('metadata-language')
        if meta_language:
            meta_language_uri = get_uri_from_label('eu_language', meta_language)
            if meta_language_uri:
                meta_dict.set_value('language', meta_language_uri)

        if iso_values.get('metadata-point-of-contact'):
            for org_object in iso_values['metadata-point-of-contact']:
                org_dict = meta_dict.new_item('contact_point')
                if 'contact-info' in org_object:
                    org_dict.set_value('email', org_object['contact-info'].get('email'))
                    org_dict.set_value('url', org_object['contact-info'].get('online-resource'))
                    # TODO: le numéro de téléphone n'est pas récupéré dans 'contact-info',
                    # "gmd:phone/gmd:CI_Telephone/gmd:voice/gco:CharacterString/text()"

        catalog_dict = meta_dict.new_item('in_catalog')

        for elem in package_dict['extras']:
        # the following are "default extras" from the
        # harvest source

            if elem['key'] == 'catalog_title':
                catalog_dict.set_value('title', elem['value'])
            
            elif elem['key'] == 'catalog_homepage':
                catalog_dict.set_value('homepage', elem['value'])
        
        # --- references ---
            elif elem['key'] == 'catalog_base_url' and name:
                landing_page = '{}/{}'.format(elem['value'], name)
                response = requests.get(landing_page)
                if response.status_code != requests.codes.ok:
                    dataset_dict.set_value('landing_page', landing_page)
                    dataset_dict.set_value('uri', landing_page)
            
            elif elem['key'] == 'attributes_base_url' and name:
                attributes_page = '{}/{}'.format(elem['value'], name)
                response = requests.get(attributes_page)
                if response.status_code != requests.codes.ok:
                    dataset_dict.set_value('attributes_page', attributes_page)

        # page

        # --- themes and keywords ---

        # --- spatial coverage ---

        # --- relations ---

        # in_series
        # in_series > uri
        # in_series > url
        # in_series > title

        # series_member
        # series_member > uri
        # series_member > url
        # series_member > title

        # --- etc. ---

        frequency = iso_values.get('frequency-of-update')
        # might either be a code or some label
        if frequency:
            frequency_uri = get_uri_from_identifier(
                'inspire_maintenance_frequency', frequency)
            if frequency_uri:
                # it was a code
                dataset_dict.set_value('accrual_periodicity', frequency_uri)
            else:
                # it wasn't a code -> try to get the code from the label
                frequency_uri = get_uri_from_label('inspire_maintenance_frequency', frequency)
                if frequency_uri:
                    dataset_dict.set_value('accrual_periodicity', frequency_uri)

        # access_rights
        # crs
        # conforms_to
        # ...

        return dataset_dict

        # OLD : to be deleted

        # adding some useful categories from iso_values
        for key in ('lineage', 'topic-category', 'equivalent-scale', 'metadata-point-of-contact', 'use-constraints', 'aggregation-info', 'temporal-extent-begin', 'temporal-extent-end', 'bbox'):
            value = iso_values[key]
            if value and isinstance(value, (list, dict)):
                package_dict[key] = json.dumps(value)
            elif value:
                package_dict[key] = value
        
        # more straight-forward serialization for dates
        value = {}
        for date_object in iso_values['dataset-reference-date']:
            value.update({date_object['type']: date_object['value']})

        if value:
            package_dict['dataset-reference-date'] = json.dumps(value)

        # get other fields from extras
        extras_to_keep = ['guid', 'metadata-date', 'resource-type', 'licence', 'spatial', 'spatial-reference-system', 'metadata-language', 'coupled-resource', 'access_constraints', 'graphic-preview-file']
        for extra in package_dict['extras'].copy():
            field = extra['key']
            if field not in extras_to_keep :
                package_dict['extras'].remove(extra)
            else:
                package_dict[field] = extra['value']
                package_dict['extras'].remove(extra)
        
        # package name should always be its guid, for easier
        # handling of packages relationships and duplicate removal
        package_dict['name'] = package_dict['guid']


        ### LXML parsing ###
        namespaces = {
           "gts": "http://www.isotc211.org/2005/gts",
           "gml": "http://www.opengis.net/gml",
           "gml32": "http://www.opengis.net/gml/3.2",
           "gmx": "http://www.isotc211.org/2005/gmx",
           "gsr": "http://www.isotc211.org/2005/gsr",
           "gss": "http://www.isotc211.org/2005/gss",
           "gco": "http://www.isotc211.org/2005/gco",
           "gmd": "http://www.isotc211.org/2005/gmd",
           "srv": "http://www.isotc211.org/2005/srv",
           "xlink": "http://www.w3.org/1999/xlink",
           "xsi": "http://www.w3.org/2001/XMLSchema-instance",
        }

        # Licence parsing
        licence_parsed = []
        for licence in xml_tree.findall("gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:useLimitation/gco:CharacterString",\
                                         namespaces):
            licence_parsed.append(etree.tostring(licence, method='text', encoding=str))
        if package_dict['licence'] == '[]':
            package_dict['licence'] = json.dumps(licence_parsed)
        
        licence_list = json.loads(package_dict['licence'])
        if licence_list:
            license_map = {
                ('etalab', 'licence'): 'etalab-2.0',
                ('licence ouverte',): 'etalab-2.0',
                ('odbl',): 'ODbL-1.0',
                ('open database license',): 'ODbL-1.0'
                }
            for licence in licence_list:
                package_license = None                
                for keywords, license_id in license_map.items() :
                    if all(w in licence.lower() for w in keywords):
                        package_license = license_id
                        break
                if package_license:
                    package_dict['license_id'] = package_license
                    break

        # Aggregate parsing
        aggregate_parsed = []
        for agg in xml_tree.findall("gmd:identificationInfo/gmd:MD_DataIdentification/gmd:aggregationInfo/gmd:MD_AggregateInformation/*/gmd:MD_Identifier/gmd:code/gco:CharacterString", namespaces):
            aggregate_parsed.append(etree.tostring(agg, method='text', encoding=str))

        package_dict['aggregate-dataset-identifier'] = json.dumps(aggregate_parsed)

        relationships_as_subject = []
        for rel_guid in aggregate_parsed:
            if rel_guid:
                rel_package = Package.get(rel_guid)
                if rel_package:
                    relationships_as_subject.append(
                        { 'object': rel_package.id, 'type': 'parent_of' }
                        )
                        # TODO: type of relationship should be infered
                        # from rel.get('association-type')
        # package_dict['relationships_as_subject'] = relationships_as_subject # fait planter le moissonnage pour l'instant...

        # print(package_dict)
        # print(iso_values)

        # more accommodating resource format identification
        # and droping every 'Unnamed resource'
        format_map = {
            ('wfs', 'wms'): 'WxS',
            ('wxs',): 'WxS',
            ('wfs',): 'WFS',
            ('wms',): 'WMS',
            ('atom',): 'ZIP',
            ('html',): 'HTML',
            ('xml',): 'XML'
            }
        format_map_url = {
            ('mapservwfs?',): 'WFS',
            ('mapserv?',): 'WMS',
            ('atomdataset',): 'ZIP',
            ('atomarchive',): 'ZIP',
            }
        for resource in package_dict['resources'].copy():
            if resource['name'] == toolkit._('Unnamed resource'):
                package_dict['resources'].remove(resource)
                continue
            if not resource['format']:
                for keywords, format_name in format_map.items() :
                    if all(w in resource['name'].lower() for w in keywords):
                        resource['format'] = format_name
                        break
            if not resource['format']:
                for keywords, format_name in format_map_url.items() :
                    if all(w in resource['url'].lower() for w in keywords):
                        resource['format'] = format_name
                        break
        # if len(aggregate_parsed)>0:
        print("package_dict: ",package_dict)
        return package_dict
        

