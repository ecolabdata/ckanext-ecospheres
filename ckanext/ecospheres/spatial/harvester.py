# encoding: utf-8

from datetime import datetime
from lxml import etree
import logging

import ckan.plugins as plugins
import ckan.plugins.toolkit as toolkit
from ckan.model.package import Package
from ckan.lib.helpers import json

from ckanext.spatial.interfaces import ISpatialHarvester
from ckanext.harvest.harvesters.base import HarvesterBase

from ckanext.ecospheres.spatial.utils import (
    build_dataset_dict_from_schema, bbox_geojson_from_coordinates,
    bbox_wkt_from_coordinates, build_attributes_page_url,
    build_catalog_page_url, extract_scheme_and_identifier
)
from ckanext.ecospheres.maps import ISO_639_2
from ckanext.ecospheres.vocabulary.reader import VocabularyReader

logger = logging.getLogger(__name__)

class FrSpatialHarvester(plugins.SingletonPlugin):
    '''Customization of spatial metadata harvest.
    
    See :py:class:`ckanext.spatial.harvesters.base.SpatialHarvester`.
    
    '''

    plugins.implements(ISpatialHarvester, inherit=True)
    
    def get_package_dict(self, context, data_dict):
        '''Add some usefull informations to package metadata dictionnary.
        
        :param context: contains a reference to the model, eg to
            perform DB queries, and the user name used for
            authorization.
        :type context: dict
        :param data_dict: available data. Contains four keys:
            * `package_dict`
               The default package_dict generated by the harvester. Modify this
               or create a brand new one.
            * `iso_values`
               The parsed ISO XML document values. These contain more fields
               that are not added by default to the ``package_dict``.
            * `xml_tree`
               The full XML etree object. If some values not present in
               ``iso_values`` are needed, these can be extracted via xpath.
            * `harvest_object`
               A ``HarvestObject`` domain object which contains a reference
               to the original metadata document (``harvest_object.content``)
               and the harvest source (``harvest_object.source``).
        :type data_dict: dict
        
        :returns: a dataset dict ready to be used by ``package_create`` or
                  ``package_update``
        :rtype: dict
        
        See :py:func:`ckanext.spatial.harvesters.base.SpatialHarvester.get_package_dict`
        and :py:func:`ckanext.spatial.interface.ISpatialHarvester.get_package_dict`.
        
        '''
        package_dict = data_dict['package_dict']
        iso_values = data_dict['iso_values'] 
        xml_tree = data_dict['xml_tree']

        language = iso_values.get('metadata-language') or 'fr'
        if len(language) > 2:
            # if possible (and as expected), the RDF language tag
            # will use the 2 letters ISO language codes instead of
            # the 3 letters ones
            language=ISO_639_2.get(language, language)
        
        dataset_dict = build_dataset_dict_from_schema('dataset', main_language=language)

        # --- various metadata to pick up from package_dict ---
        for target_field, package_field in {
            # dataset_dict key -> package_dict key
            'owner_org': 'owner_org'
        }.items():
            dataset_dict.set_value(target_field, package_dict.get(package_field))

        # --- various metadata to pick up from package_dict's extras ---
        extras_map = {
            # ckanext-spatial extras key -> dataset_dict key
            'graphic-preview-file': 'graphic_preview'   
        }
        for elem in package_dict['extras']:
            if elem['key'] in extras_map:
                dataset_dict.set_value(extras_map[elem['key']], elem['value'])

        # --- various metadata to pick up from iso_values ---
        for target_field, iso_field in {
            # dataset_dict key -> iso_values key
            'title': 'title',
            'notes': 'abstract',
            'name': 'guid',
            'provenance': 'lineage',
            'provenance': 'maintenance-note', # TODO: provenance or version_info ? 
            'provenance': 'purpose',
            'identifier': 'unique-resource-identifier'
        }.items():
            dataset_dict.set_value(target_field, iso_values.get(iso_field))
        # NB: package name should always be its guid, for easier
        # handling of packages relationships and duplicate removal

        if not dataset_dict.get('title'):
            dataset_dict.set_value('title', iso_values.get('alternate-title'))

        name = dataset_dict.get('name')

        # --- dates ----
        if iso_values.get('dataset-reference-date'):
            type_date_map = {
                # ISO CI_DateTypeCode -> dataset_dict key
                'creation': 'created',
                'publication': 'issued',
                'revision': 'modified',
            }
            for date_object in iso_values['dataset-reference-date']:
                if date_object['type'] in type_date_map:
                    dataset_dict.set_value(type_date_map[date_object['type']], date_object['value'])
        
        if iso_values.get('temporal-extent-begin') or iso_values.get('temporal-extent-end'):
            temporal_dict = dataset_dict.new_item('temporal')
            temporal_dict.set_value('start_date', iso_values.get('temporal-extent-begin'))
            temporal_dict.set_value('end_date', iso_values.get('temporal-extent-end'))

        # --- organizations ---
        if iso_values.get('responsible-organisation'):
            base_role_map = {
                # ISO CI_RoleCode -> dataset_dict key
                'owner': 'rights_holder',
                'publisher': 'publisher',
                'author': 'creator',
                'pointOfContact': 'contact_point'
                }
            for org_object in iso_values['responsible-organisation']:
                if not 'role' in org_object or not 'organisation-name' in org_object:
                    continue
                org_role = org_object['role']
                if org_role in base_role_map:
                    org_dict = dataset_dict.new_item(base_role_map[org_role])
                else:
                    role_uri = VocabularyReader.get_uri_from_label('inspire_role', org_role)
                    if role_uri:
                        qa_dict = dataset_dict.new_item('qualified_attribution')
                        qa_dict.set_value('had_role', role_uri)
                        org_dict = qa_dict.new_item('agent')
                    else:
                        continue
                org_dict.set_value('name', org_object['organisation-name'])
                if 'contact-info' in org_object:
                    org_dict.set_value('email', org_object['contact-info'].get('email'))
                    org_dict.set_value('url', org_object['contact-info'].get('online-resource'))
        
        # --- metadata's metadata ---
        '''
        meta_dict = dataset_dict.new_item('is_primary_topic_of')
        meta_dict.set_value('harvested', datetime.now().astimezone().isoformat())
        meta_dict.set_value('modified', iso_values.get('metadata-date'))
        meta_dict.set_value('identifier', name)

        meta_language = iso_values.get('metadata-language')
        if meta_language:
            meta_language_uri =  VocabularyReader.get_uri_from_label('eu_language', meta_language)
            if meta_language_uri:
                meta_dict.set_value('language', meta_language_uri)

        if iso_values.get('metadata-point-of-contact'):
            for org_object in iso_values['metadata-point-of-contact']:
                org_dict = meta_dict.new_item('contact_point')
                if 'contact-info' in org_object:
                    org_dict.set_value('email', org_object['contact-info'].get('email'))
                    org_dict.set_value('url', org_object['contact-info'].get('online-resource'))
                    # TODO: le numéro de téléphone n'est pas récupéré dans 'contact-info',
                    # "gmd:phone/gmd:CI_Telephone/gmd:voice/gco:CharacterString/text()"

        catalog_dict = meta_dict.new_item('in_catalog')

        for elem in package_dict['extras']:
        # the following are "default extras" from the
        # harvest source, not harvested metadata

            if elem['key'] == 'catalog_title':
                catalog_dict.set_value('title', elem['value'])
            
            elif elem['key'] == 'catalog_homepage':
                catalog_dict.set_value('homepage', elem['value'])
        
        # --- references / documentation ---
            elif elem['key'] == 'catalog_base_url' and name:
                landing_page = build_catalog_page_url(elem['value'], name)
                if landing_page:
                    dataset_dict.set_value('landing_page', landing_page)
                    dataset_dict.set_value('uri', landing_page)
            
            elif elem['key'] == 'attributes_base_url' and name:
                attributes_page = build_attributes_page_url(
                    elem['value'], name
                )
                if attributes_page:
                    dataset_dict.set_value('attributes_page', attributes_page)
        '''
        # page
        # TODO

        # --- themes and keywords ---

        iso_themes = iso_values.get('keyword-inspire-theme', [])
        if iso_themes:
            for theme in iso_themes:
                theme_uri =  VocabularyReader.get_uri_from_label(
                    'inspire_theme', theme
                )
                if theme_uri:
                    dataset_dict.set_value('theme', theme_uri)
                
        iso_topic_categories = iso_values.get('topic-category', [])
        if iso_topic_categories:
            for iso_topic_category in iso_topic_categories:
                topic_category_uri = VocabularyReader.get_uri_from_label(
                    'inspire_topic_category', iso_topic_category
                )
                if topic_category_uri:
                    dataset_dict.set_value('theme', topic_category_uri)
        
        for iso_keyword in iso_values.get('keywords', []):
            dataset_dict.set_value('free_tags', iso_keyword.get('keyword'))

        words = (
            iso_themes + iso_topic_categories
            + dataset_dict.get_values('free_tags')
            + dataset_dict.get_values('title')
        )
        if words:
            for word in words:
                category_uri = VocabularyReader.get_uri_from_label(
                    'ecospheres_theme', word
                )
                if category_uri:
                    dataset_dict.set_value('category', category_uri)                
            category_uris = VocabularyReader.get_uris_from_regexp(
                'ecospheres_theme', words
            )
            if category_uris:
                dataset_dict.set_value('category', category_uris)
            for category_uri in category_uris:
                parent_category_uri = VocabularyReader.get_parents(
                    'ecospheres_theme', category_uri
                )
                if parent_category_uri:
                    dataset_dict.set_value('category', parent_category_uri)
            # NB: the method makes sure no value is listed
            # more than once, hence not test is necessary here

        # --- spatial coverage ---

        # bounding box
        iso_bboxes = iso_values.get('bbox')
        for iso_bbox in iso_bboxes:
            if iso_bbox:
                coordinates = (
                    iso_bbox.get('west'),
                    iso_bbox.get('east'),
                    iso_bbox.get('south'),
                    iso_bbox.get('north')
                )
                if all(coordinate is not None for coordinate in coordinates):
                    wkt = bbox_wkt_from_coordinates(*coordinates)
                    dataset_dict.set_value('bbox', wkt)
                    geojson = bbox_geojson_from_coordinates(*coordinates)
                    dataset_dict.set_value('spatial', geojson)
                    break # other bboxes will be lost

        # spatial_coverage and territory
        iso_extents = (
            iso_values.get('extent-free-text', [])
            + iso_values.get('extent-controlled', [])
        )
        # extent-controlled is defined in ckanext-spatial's model, but
        # doesn't have any associated search path for now. It's added
        # here in case future updates put it to use.
        
        for iso_extent in iso_extents:
            spatial_coverage_uri = None
            if VocabularyReader.is_known_uri(
                'eu_administrative_territory_unit', iso_extent
                ):
                # actually the less likely vocabulary to be found
                # in INSPIRE catalogs, but it's much smaller, so
                # it makes sense to try it first.
                spatial_coverage_uri = iso_extent
                spatial_coverage_voc = 'eu_administrative_territory_unit'
            elif VocabularyReader.is_known_uri(
                'insee_official_geographic_code', iso_extent
                ):
                spatial_coverage_uri = iso_extent
                spatial_coverage_voc = 'insee_official_geographic_code'
            elif len(iso_extent) > 2:
                # excluding short strings for now, because of the possible
                # mix up between codes of different types of territories
                extent_scheme, extent_id = extract_scheme_and_identifier(iso_extent)
                spatial_coverage_uri = VocabularyReader.get_uri_from_label(
                        'insee_official_geographic_code', extent_id
                    )
                spatial_coverage_voc = 'insee_official_geographic_code'
            
            if spatial_coverage_uri:
                spatial_coverage_dict = dataset_dict.new_item('spatial_coverage')
                spatial_coverage_dict.set_value('uri', spatial_coverage_uri)
                dataset_dict.set_value(
                    'territory',
                    VocabularyReader.get_ecospheres_territory(
                        spatial_coverage_voc, spatial_coverage_uri
                    )
                )
            elif extent_scheme:
                if VocabularyReader.is_known_uri(
                    'insee_gazetteer', extent_scheme
                ):
                    spatial_coverage_dict = dataset_dict.new_item('spatial_coverage')
                    spatial_coverage_dict.set_value('in_scheme', extent_scheme)
                    spatial_coverage_dict.set_value('identifier', extent_id)
                # if the gazetteer can't be recognized either, the info is lost
            else:
                # whatever it is, it's not an URI, so should hopefully be readable
                # by a human being.
                spatial_coverage_dict = dataset_dict.new_item('spatial_coverage')
                spatial_coverage_dict.set_value('label', extent_id)

            if not dataset_dict.get_values('territory'):
                # get the territories from the organization
                # TODO
                pass

        # --- relations ---

        # in_series
        # in_series > uri
        # in_series > url
        # in_series > title

        # series_member
        # series_member > uri
        # series_member > url
        # series_member > title

        # --- etc. ---

        frequency = iso_values.get('frequency-of-update')
        # might either be a code or some label, but codes are
        # stored as alternative labels, so get_uri_from_label
        # will work in both cases
        if frequency:
            frequency_uri =  VocabularyReader.get_uri_from_label(
                'inspire_maintenance_frequency', frequency
            )
            if frequency_uri:
                dataset_dict.set_value('accrual_periodicity', frequency_uri)

        states = iso_values.get('progress')
        # apparently admits more than one value, when DCAT does not
        # only the last valid value will be stored
        if states:
            for state in states:
                state_uri =  VocabularyReader.get_uri_from_label(
                    'iso19139_progress_code', state
                )
                if state_uri:
                    dataset_dict.set_value('status', state_uri)

        # access_rights
        # crs
        # conforms_to
        # ...

        return dataset_dict.flat()

        # OLD : to be deleted

        # adding some useful categories from iso_values
        for key in ('lineage', 'topic-category', 'equivalent-scale', 'metadata-point-of-contact', 'use-constraints', 'aggregation-info', 'temporal-extent-begin', 'temporal-extent-end', 'bbox'):
            value = iso_values[key]
            if value and isinstance(value, (list, dict)):
                package_dict[key] = json.dumps(value)
            elif value:
                package_dict[key] = value
        
        # more straight-forward serialization for dates
        value = {}
        for date_object in iso_values['dataset-reference-date']:
            value.update({date_object['type']: date_object['value']})

        if value:
            package_dict['dataset-reference-date'] = json.dumps(value)

        # get other fields from extras
        extras_to_keep = ['guid', 'metadata-date', 'resource-type', 'licence', 'spatial', 'spatial-reference-system', 'metadata-language', 'coupled-resource', 'access_constraints', 'graphic-preview-file']
        for extra in package_dict['extras'].copy():
            field = extra['key']
            if field not in extras_to_keep :
                package_dict['extras'].remove(extra)
            else:
                package_dict[field] = extra['value']
                package_dict['extras'].remove(extra)
        
        # package name should always be its guid, for easier
        # handling of packages relationships and duplicate removal
        package_dict['name'] = package_dict['guid']


        ### LXML parsing ###
        namespaces = {
           "gts": "http://www.isotc211.org/2005/gts",
           "gml": "http://www.opengis.net/gml",
           "gml32": "http://www.opengis.net/gml/3.2",
           "gmx": "http://www.isotc211.org/2005/gmx",
           "gsr": "http://www.isotc211.org/2005/gsr",
           "gss": "http://www.isotc211.org/2005/gss",
           "gco": "http://www.isotc211.org/2005/gco",
           "gmd": "http://www.isotc211.org/2005/gmd",
           "srv": "http://www.isotc211.org/2005/srv",
           "xlink": "http://www.w3.org/1999/xlink",
           "xsi": "http://www.w3.org/2001/XMLSchema-instance",
        }

        # Licence parsing
        licence_parsed = []
        for licence in xml_tree.findall("gmd:identificationInfo/gmd:MD_DataIdentification/gmd:resourceConstraints/gmd:MD_LegalConstraints/gmd:useLimitation/gco:CharacterString",\
                                         namespaces):
            licence_parsed.append(etree.tostring(licence, method='text', encoding=str))
        if package_dict['licence'] == '[]':
            package_dict['licence'] = json.dumps(licence_parsed)
        
        licence_list = json.loads(package_dict['licence'])
        if licence_list:
            license_map = {
                ('etalab', 'licence'): 'etalab-2.0',
                ('licence ouverte',): 'etalab-2.0',
                ('odbl',): 'ODbL-1.0',
                ('open database license',): 'ODbL-1.0'
                }
            for licence in licence_list:
                package_license = None                
                for keywords, license_id in license_map.items() :
                    if all(w in licence.lower() for w in keywords):
                        package_license = license_id
                        break
                if package_license:
                    package_dict['license_id'] = package_license
                    break

        # Aggregate parsing
        aggregate_parsed = []
        for agg in xml_tree.findall("gmd:identificationInfo/gmd:MD_DataIdentification/gmd:aggregationInfo/gmd:MD_AggregateInformation/*/gmd:MD_Identifier/gmd:code/gco:CharacterString", namespaces):
            aggregate_parsed.append(etree.tostring(agg, method='text', encoding=str))

        package_dict['aggregate-dataset-identifier'] = json.dumps(aggregate_parsed)

        relationships_as_subject = []
        for rel_guid in aggregate_parsed:
            if rel_guid:
                rel_package = Package.get(rel_guid)
                if rel_package:
                    relationships_as_subject.append(
                        { 'object': rel_package.id, 'type': 'parent_of' }
                        )
                        # TODO: type of relationship should be infered
                        # from rel.get('association-type')
        # package_dict['relationships_as_subject'] = relationships_as_subject # fait planter le moissonnage pour l'instant...

        # print(package_dict)
        # print(iso_values)

        # more accommodating resource format identification
        # and droping every 'Unnamed resource'
        format_map = {
            ('wfs', 'wms'): 'WxS',
            ('wxs',): 'WxS',
            ('wfs',): 'WFS',
            ('wms',): 'WMS',
            ('atom',): 'ZIP',
            ('html',): 'HTML',
            ('xml',): 'XML'
            }
        format_map_url = {
            ('mapservwfs?',): 'WFS',
            ('mapserv?',): 'WMS',
            ('atomdataset',): 'ZIP',
            ('atomarchive',): 'ZIP',
            }
        for resource in package_dict['resources'].copy():
            if resource['name'] == toolkit._('Unnamed resource'):
                package_dict['resources'].remove(resource)
                continue
            if not resource['format']:
                for keywords, format_name in format_map.items() :
                    if all(w in resource['name'].lower() for w in keywords):
                        resource['format'] = format_name
                        break
            if not resource['format']:
                for keywords, format_name in format_map_url.items() :
                    if all(w in resource['url'].lower() for w in keywords):
                        resource['format'] = format_name
                        break
        # if len(aggregate_parsed)>0:
        print("package_dict: ",package_dict)
        return package_dict
        

